{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet18 구현에 관한 BasicBlock 클래스 정의\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # 3x3 필터를 사용 (피처맵 크기를 줄일 때는 stride 값 조절)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity인 경우. 비어있는 컨테이너이며, forward에서 x를 입력받아 residual 연산을 할 수 있도록 만든다.\n",
    "        if stride != 1: # stride가 1이 아니라면, identity mapping이 아닌 경우이다. 이 경우에는 projection을 수행하여 forward에서 계산될 수 있도록 만든다.\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out)) # 여기까지가 F(x)\n",
    "        out += self.shortcut(x) # F(x) + x\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # CIFAR10: 3 x 32 x 32\n",
    "        # 64개의 3x3 필터(filter)를 사용\n",
    "        # (이미지의 첫 시작 채널 개수는 RGB라 3개, 그 다음은 64개의 필터를 만들 것이므로 64개로 out_channel을 설정한다)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) # shape=3x32x32 -> shape= 64x32x32\n",
    "        self.bn = nn.BatchNorm2d(64) # BatchNorm의 첫 번째 매개변수는 입력 데이터의 채널 수이다.\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) # shape= 64x32x32 -> shape= 64x32x32\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) # shape= 64x32x32 -> shape= 128x16x16\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) # shape= 128x16x16 -> shape= 256x8x8\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) # shape= 256x8x8 -> shape= 512x4x4\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        strides = [1]\n",
    "        strides = [2, 1]\n",
    "        strides = [2, 1]\n",
    "        strides = [2, 1]\n",
    "        \"\"\"\n",
    "        strides = [stride] + ([1] * (num_blocks - 1))\n",
    "\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride)) # 64, 64\n",
    "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    # Outputsize = Inputsize / Poolingsize\n",
    "    # tensor.view()\n",
    "    def forward(self, x):\n",
    "        out = self.bn(self.conv1(x))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4) # input tensor=512x4x4, kernel_size=4 -> 512x1x1\n",
    "        out = out.view(out.size(0), -1) # 512x1x1 -> 512x1\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ResNet18 함수 정의\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋 다운 및 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transfrom_test = transforms.Compose([\n",
    "    transforms.ToTensor()   \n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transfrom_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 및 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet18_cifar10.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total_data_num = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # optimizer.zero_grad()\n",
    "        for param in net.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        benign_outputs = net(inputs)\n",
    "        loss = criterion(benign_outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted_idx = benign_outputs.max(1)\n",
    "\n",
    "        total_data_num += targets.size(0) # 128\n",
    "        correct += predicted_idx.eq(targets).sum().item() # 예측이 맞은 데이터의 개수를 correct에 더함\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current benign train accuracy:', str(predicted_idx.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "    \n",
    "    total_train_accuracy = 100. * correct / total_data_num\n",
    "    print('\\nTotal benign train accuracy:', total_train_accuracy)\n",
    "    print('Total benign train loss:', train_loss)\n",
    "\n",
    "    return total_train_accuracy\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "\n",
    "    total_test_accuracy = correct / total * 100.\n",
    "    print('\\nTest acuracy:', total_test_accuracy)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "\n",
    "    return total_test_accuracy\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch): # Learning rate scheduler\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.1015625\n",
      "Current benign train loss: 2.334784984588623\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.3125\n",
      "Current benign train loss: 1.9909483194351196\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 1.79517662525177\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.34375\n",
      "Current benign train loss: 1.6984061002731323\n",
      "\n",
      "Total benign train accuracy: 30.284\n",
      "Total benign train loss: 769.2804585695267\n",
      "\n",
      "[ Test epoch: 0]\n",
      "\n",
      "Test acuracy: 36.36\n",
      "Test average loss: 0.01884780024290085\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 1.5822407007217407\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.4296875\n",
      "Current benign train loss: 1.5059665441513062\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.4609375\n",
      "Current benign train loss: 1.4823895692825317\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.40625\n",
      "Current benign train loss: 1.510015606880188\n",
      "\n",
      "Total benign train accuracy: 46.696\n",
      "Total benign train loss: 568.0980999469757\n",
      "\n",
      "[ Test epoch: 1]\n",
      "\n",
      "Test acuracy: 48.17\n",
      "Test average loss: 0.014499224662780762\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.5078125\n",
      "Current benign train loss: 1.2908339500427246\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.5859375\n",
      "Current benign train loss: 1.211663007736206\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6484375\n",
      "Current benign train loss: 1.0718456506729126\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 1.0967189073562622\n",
      "\n",
      "Total benign train accuracy: 57.668\n",
      "Total benign train loss: 459.43678945302963\n",
      "\n",
      "[ Test epoch: 2]\n",
      "\n",
      "Test acuracy: 54.269999999999996\n",
      "Test average loss: 0.012742447352409363\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 1.1369515657424927\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 0.9321380257606506\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.578125\n",
      "Current benign train loss: 1.2125835418701172\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.9658832550048828\n",
      "\n",
      "Total benign train accuracy: 65.634\n",
      "Total benign train loss: 378.337649166584\n",
      "\n",
      "[ Test epoch: 3]\n",
      "\n",
      "Test acuracy: 60.77\n",
      "Test average loss: 0.011805454498529435\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.885817289352417\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 0.8793550729751587\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 0.6626450419425964\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.6274509429931641\n",
      "\n",
      "Total benign train accuracy: 72.02\n",
      "Total benign train loss: 312.72384214401245\n",
      "\n",
      "[ Test epoch: 4]\n",
      "\n",
      "Test acuracy: 68.19\n",
      "Test average loss: 0.00935948781967163\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 0.6635148525238037\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 0.8629714250564575\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 0.7132285833358765\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.6422005295753479\n",
      "\n",
      "Total benign train accuracy: 76.298\n",
      "Total benign train loss: 264.41912883520126\n",
      "\n",
      "[ Test epoch: 5]\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(0, 200):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_accuracies.append(train(epoch))\n",
    "    test_accuracies.append(test(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.plot(range(0, 200), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(0, 200), test_accuracies, label='Test Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cifar10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
